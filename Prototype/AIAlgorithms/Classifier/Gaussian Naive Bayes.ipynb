{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c2df5a2",
   "metadata": {},
   "source": [
    "🔹 1. Nguyên lý hoạt động (không công thức nặng!)\n",
    "🎯 Mục tiêu:\n",
    "Dự đoán xác suất một mẫu thuộc về mỗi lớp, rồi chọn lớp có xác suất cao nhất.\n",
    "\n",
    "🧠 Ý tưởng chính — dựa trên định lý Bayes:\n",
    "\"Xác suất một điều kiện xảy ra nếu biết một bằng chứng\" \n",
    "\n",
    "Ví dụ:\n",
    "\n",
    "“Nếu email có từ ‘miễn phí’, thì xác suất nó là spam là bao nhiêu?” \n",
    "\n",
    "🤔 Nhưng “Naive” (ngây thơ) ở đâu?\n",
    "Thuật toán giả định các đặc trưng là ĐỘC LẬP với nhau.\n",
    "Ví dụ: sự xuất hiện của từ “miễn phí” không ảnh hưởng đến từ “trúng thưởng”.\n",
    "Trong thực tế, điều này rất hiếm khi đúng → nên gọi là “ngây thơ”.\n",
    "Nhưng kỳ lạ thay: dù giả định sai, Naive Bayes vẫn hoạt động rất tốt!\n",
    "💡 Cách nó “học”:\n",
    "Không học trọng số như Logistic Regression.\n",
    "Mà đếm tần suất trong dữ liệu huấn luyện:\n",
    "Tỷ lệ email spam trong toàn bộ dữ liệu → xác suất prior\n",
    "Tỷ lệ từ “miễn phí” xuất hiện trong email spam → khả năng xuất hiện từ đó nếu là spam\n",
    "→ Khi có email mới, nó kết hợp các xác suất này để ra dự đoán.\n",
    "\n",
    "✅ Ưu điểm:\n",
    "Rất nhanh, ít tốn bộ nhớ.\n",
    "Hiệu quả cao với dữ liệu văn bản (text classification).\n",
    "Hoạt động tốt ngay cả khi dữ liệu huấn luyện ít.\n",
    "❌ Hạn chế:\n",
    "Giả định độc lập giữa các đặc trưng → có thể sai nặng nếu đặc trưng liên quan chặt (ví dụ: chiều cao & cân nặng).\n",
    "Không cho ra mô hình phức tạp → khó bắt được mối quan hệ ẩn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd502d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleGaussianNB:\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        # Lưu thống kê cho mỗi lớp: mean và variance của từng đặc trưng\n",
    "        self.mean = np.zeros((n_classes, n_features))\n",
    "        self.var = np.zeros((n_classes, n_features))\n",
    "        self.priors = np.zeros(n_classes)\n",
    "        \n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.mean[idx, :] = X_c.mean(axis=0)\n",
    "            self.var[idx, :] = X_c.var(axis=0)\n",
    "            self.priors[idx] = X_c.shape[0] / float(n_samples)\n",
    "    \n",
    "    def _gaussian_probability(self, class_idx, x):\n",
    "        \"\"\"Tính xác suất x thuộc lớp class_idx theo phân phối chuẩn\"\"\"\n",
    "        mean = self.mean[class_idx]\n",
    "        var = self.var[class_idx]\n",
    "        # Tránh chia cho 0\n",
    "        var = np.clip(var, 1e-9, None)\n",
    "        # Công thức phân phối chuẩn (không cần hằng số chuẩn hóa)\n",
    "        numerator = np.exp(-0.5 * ((x - mean) ** 2) / var)\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return np.prod(numerator / denominator)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            posteriors = []\n",
    "            for idx, c in enumerate(self.classes):\n",
    "                # P(class) * P(x1|class) * P(x2|class) * ...\n",
    "                prior = np.log(self.priors[idx])  # dùng log để tránh underflow\n",
    "                likelihood = np.sum(np.log(self._gaussian_probability(idx, x) + 1e-9))\n",
    "                posterior = prior + likelihood\n",
    "                posteriors.append(posterior)\n",
    "            # Chọn lớp có posterior lớn nhất\n",
    "            y_pred.append(self.classes[np.argmax(posteriors)])\n",
    "        return np.array(y_pred)\n",
    "\n",
    "# --- Thử nghiệm ---\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Dùng Iris (đặc trưng là số thực → phù hợp Gaussian NB)\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Huấn luyện mô hình tự code\n",
    "nb_model = SimpleGaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"✅ Độ chính xác (Naive Bayes tự code): {acc:.2%}\")\n",
    "\n",
    "# So sánh với sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "sk_nb = GaussianNB()\n",
    "sk_nb.fit(X_train, y_train)\n",
    "sk_acc = sk_nb.score(X_test, y_test)\n",
    "print(f\"🔍 Sklearn GaussianNB độ chính xác: {sk_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b029d1f3",
   "metadata": {},
   "source": [
    " 3. Khi nào dùng Naive Bayes?\n",
    "Phân loại văn bản\n",
    ": spam detection, phân tích cảm xúc, phân loại chủ đề\n",
    "Dữ liệu có\n",
    "mối quan hệ mạnh giữa các đặc trưng\n",
    "(ví dụ: ảnh pixel lân cận)\n",
    "Dữ liệu huấn luyện ít\n",
    "nhưng số đặc trưng lớn (text có hàng nghìn từ)\n",
    "Bạn cần\n",
    "ước lượng xác suất chính xác tuyệt đối\n",
    "(NB thường cho xác suất bị lệch)\n",
    "Cần\n",
    "mô hình cực nhanh\n",
    "để huấn luyện & dự đoán\n",
    "Đặc trưng\n",
    "không tuân theo phân phối chuẩn\n",
    "(với Gaussian NB)\n",
    "Làm\n",
    "baseline đơn giản\n",
    "cho bài toán phân loại\n",
    "Bài toán yêu cầu\n",
    "giải thích chi tiết mối quan hệ giữa đặc trưng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d8bfbb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
