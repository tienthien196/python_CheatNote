{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e3b4af",
   "metadata": {},
   "source": [
    "Gi·∫£ s·ª≠ b·∫°n d·ª± ƒëo√°n c√≥ ƒë·∫≠u ph·ªèng v·∫•n kh√¥ng d·ª±a tr√™n:\n",
    "\n",
    "S·ªë nƒÉm kinh nghi·ªám\n",
    "ƒêi·ªÉm ph·ªèng v·∫•n k·ªπ thu·∫≠t\n",
    "‚Üí Logistic Regression h·ªçc ƒë∆∞·ª£c:\n",
    "\n",
    "\"N·∫øu kinh nghi·ªám ‚â• 3 nƒÉm v√† ƒëi·ªÉm ‚â• 7 ‚Üí x√°c su·∫•t ƒë·∫≠u ~90%\" \n",
    "\n",
    "‚Üí N√≥ kh√¥ng v·∫Ω ƒë∆∞·ªùng th·∫≥ng ph√¢n chia c·ª©ng, m√† ∆∞·ªõc l∆∞·ª£ng x√°c su·∫•t m·ªÅm.\n",
    "\n",
    "‚úÖ ∆Øu ƒëi·ªÉm:\n",
    "ƒê∆°n gi·∫£n, nhanh, d·ªÖ hi·ªÉu.\n",
    "Cho x√°c su·∫•t ƒë·∫ßu ra (r·∫•t h·ªØu √≠ch!).\n",
    "L√†m n·ªÅn cho m·∫°ng neural (perceptron = logistic regression + hidden layer).\n",
    "‚ùå H·∫°n ch·∫ø:\n",
    "Ch·ªâ hi·ªáu qu·∫£ khi m·ªëi quan h·ªá g·∫ßn tuy·∫øn t√≠nh.\n",
    "Kh√¥ng x·ª≠ l√Ω t·ªët ƒë·∫∑c tr∆∞ng phi tuy·∫øn (tr·ª´ khi b·∫°n t·ª± t·∫°o ƒë·∫∑c tr∆∞ng m·ªõi).\n",
    "\n",
    "\n",
    "Logistic Regression = h·ªìi quy tuy·∫øn t√≠nh + h√†m sigmoid ‚Üí ph√¢n lo·∫°i nh·ªã ph√¢n.\n",
    "Kh√¥ng c·∫ßn c√¥ng th·ª©c ph·ª©c t·∫°p ‚Äî ch·ªâ c·∫ßn hi·ªÉu: k·∫øt h·ª£p ƒë·∫∑c tr∆∞ng ‚Üí ƒëi·ªÉm s·ªë ‚Üí x√°c su·∫•t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "229eafef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê·ªô ch√≠nh x√°c (t·ª± code): 88.00%\n",
      "üîç Sklearn ƒë·ªô ch√≠nh x√°c: 88.33%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.1, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        # H√†m sigmoid: bi·∫øn s·ªë th·ª±c th√†nh x√°c su·∫•t [0, 1]\n",
    "        # Tr√°nh overflow b·∫±ng c√°ch gi·ªõi h·∫°n z\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        # Kh·ªüi t·∫°o tr·ªçng s·ªë ng·∫´u nhi√™n nh·ªè\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Gradient Descent: l·∫∑p ƒë·ªÉ c·∫£i thi·ªán m√¥ h√¨nh\n",
    "        for _ in range(self.n_iters):\n",
    "            # D·ª± ƒëo√°n x√°c su·∫•t\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self._sigmoid(linear_model)\n",
    "\n",
    "            # T√≠nh gradient (h∆∞·ªõng ƒëi ƒë·ªÉ gi·∫£m l·ªói)\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "\n",
    "            # C·∫≠p nh·∫≠t tr·ªçng s·ªë\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        return self._sigmoid(linear_model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) > 0.5).astype(int)\n",
    "\n",
    "# --- Th·ª≠ nghi·ªám v·ªõi d·ªØ li·ªáu th·∫≠t ---\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu nh·ªã ph√¢n ƒë∆°n gi·∫£n\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_informative=2,\n",
    "                           n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Hu·∫•n luy·ªán m√¥ h√¨nh t·ª± code\n",
    "model = SimpleLogisticRegression(learning_rate=0.1, n_iters=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# D·ª± ƒëo√°n\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"‚úÖ ƒê·ªô ch√≠nh x√°c (t·ª± code): {acc:.2%}\")\n",
    "\n",
    "# So s√°nh v·ªõi sklearn (ƒë·ªÉ ki·ªÉm tra)\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLR\n",
    "sk_model = SklearnLR()\n",
    "sk_model.fit(X_train, y_train)\n",
    "sk_acc = sk_model.score(X_test, y_test)\n",
    "print(f\"üîç Sklearn ƒë·ªô ch√≠nh x√°c: {sk_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7816d892",
   "metadata": {},
   "source": [
    "üîπ 3. Khi n√†o d√πng Logistic Regression?\n",
    "B·∫°n c·∫ßn\n",
    "x√°c su·∫•t ƒë·∫ßu ra\n",
    "(kh√¥ng ch·ªâ nh√£n)\n",
    "D·ªØ li·ªáu c√≥\n",
    "m·ªëi quan h·ªá phi tuy·∫øn ph·ª©c t·∫°p\n",
    "(v√≠ d·ª•: h√¨nh tr√≤n l·ªìng nhau)\n",
    "D·ªØ li·ªáu\n",
    "√≠t nhi·ªÖu\n",
    ",\n",
    "g·∫ßn ph√¢n t√°ch tuy·∫øn t√≠nh\n",
    "S·ªë ƒë·∫∑c tr∆∞ng\n",
    "r·∫•t l·ªõn\n",
    "m√† kh√¥ng c√≥ regularization (d·ªÖ overfit)\n",
    "B·∫°n c·∫ßn m√¥ h√¨nh\n",
    "ƒë∆°n gi·∫£n, gi·∫£i th√≠ch ƒë∆∞·ª£c\n",
    "(tr·ªçng s·ªë = m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng)\n",
    "B√†i to√°n\n",
    "·∫£nh, √¢m thanh, vƒÉn b·∫£n th√¥\n",
    "(h√£y d√πng CNN/RNN thay v√¨ LR)\n",
    "L√†m\n",
    "baseline\n",
    "tr∆∞·ªõc khi th·ª≠ m√¥ h√¨nh ph·ª©c t·∫°p\n",
    "C·∫ßn\n",
    "ƒë·ªô ch√≠nh x√°c c·ª±c cao\n",
    "tr√™n d·ªØ li·ªáu phi tuy·∫øn ‚Üí d√πng XGBoost ho·∫∑c Neural Net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4899b6b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
