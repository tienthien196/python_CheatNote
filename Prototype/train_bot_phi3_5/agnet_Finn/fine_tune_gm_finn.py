# fine_tune_gm_finn.py ‚Äî T∆Ø∆†NG TH√çCH V·ªöI trl >= 0.8.0
import os
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer
from datasets import load_dataset
import torch

# === C·∫•u h√¨nh ===
MODEL_NAME = "Qwen/Qwen3-1.7B"
DATASET_PATH = r"E:\1_test_Src\src\python\python_cheat\Project\train_bot_phi3_5\agnet_Finn\gm_finn_persona_pro_full.json"
OUTPUT_DIR = "./gm_finn_qwen3_lora"

# === T·∫£i tokenizer ===
print("ƒêang t·∫£i tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# === C·∫•u h√¨nh 4-bit quantization ===
print("Thi·∫øt l·∫≠p quantization 4-bit...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

# === T·∫£i model ===
print("ƒêang t·∫£i model (4-bit)...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.float16,  # C·∫£nh b√°o "deprecated" nh∆∞ng v·∫´n ho·∫°t ƒë·ªông
)

model = prepare_model_for_kbit_training(model)

# === C·∫•u h√¨nh LoRA ===
print("Thi·∫øt l·∫≠p c·∫•u h√¨nh LoRA...")
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# === T·∫£i dataset ===
print("ƒêang t·∫£i dataset...")
dataset = load_dataset("json", data_files=DATASET_PATH, split="train")
print("M·∫´u ƒë·∫ßu ti√™n:", dataset[0])

# H√†m ƒë·ªãnh d·∫°ng prompt
def formatting_prompts_func(example):
    # example l√† dict: {"messages": [...]}
    # Ch√∫ng ta c·∫ßn l·∫•y LIST messages v√† √°p d·ª•ng template
    messages = example["messages"]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=False,
        enable_thinking=False
    )
    return [text]  # SFTTrainer mong ƒë·ª£i list (d√π ch·ªâ 1 chu·ªói)

# === C·∫•u h√¨nh hu·∫•n luy·ªán ===
print("Thi·∫øt l·∫≠p tham s·ªë hu·∫•n luy·ªán...")
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    optim="paged_adamw_32bit",
    learning_rate=2e-4,
    num_train_epochs=5,
    logging_steps=5,
    save_steps=20,
    fp16=True,
    gradient_checkpointing=True,
    remove_unused_columns=False,
    report_to="none",
    save_total_limit=2,
)

# === Kh·ªüi t·∫°o SFTTrainer ‚Äî KH√îNG D√ôNG 'max_seq_length' v√† 'tokenizer' ===
print("Kh·ªüi t·∫°o SFTTrainer...")
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    formatting_func=formatting_prompts_func,
    # ‚úÖ ƒê√É X√ìA: max_seq_length=512
    # ‚úÖ ƒê√É X√ìA: tokenizer=tokenizer  ‚Üê ƒë√¢y l√† nguy√™n nh√¢n l·ªói m·ªõi
)

# === Hu·∫•n luy·ªán ===
print("\nüöÄ B·∫Øt ƒë·∫ßu fine-tuning...")
trainer.train()

# === L∆∞u k·∫øt qu·∫£ ===
print("ƒêang l∆∞u LoRA adapter...")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print(f"\n‚úÖ Ho√†n t·∫•t! K·∫øt qu·∫£ l∆∞u t·∫°i: {OUTPUT_DIR}")