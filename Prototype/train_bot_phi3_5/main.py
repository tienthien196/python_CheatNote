import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# === LOAD MODEL 1 Láº¦N DUY NHáº¤T ===
print("Loading model... (this takes a while)")
torch.random.manual_seed(0)

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3.5-mini-instruct",
    device_map="cuda",
    torch_dtype=torch.float16,  # tá»‘t hÆ¡n "auto" khi dÃ¹ng GPU
    trust_remote_code=True,
)
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3.5-mini-instruct")

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
)

# === Lá»ŠCH Sá»¬ Há»˜I THOáº I ===
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."}
]

print("\nâœ… Model loaded! Type 'quit' to exit.\n")

# === VÃ’NG Láº¶P NHáº¬P CÃ‚U Há»I ===
while True:
    user_input = input("ğŸ‘¤ You: ").strip()
    if user_input.lower() in ["quit", "exit", "q"]:
        print("ğŸ‘‹ Goodbye!")
        break

    # ThÃªm tin nháº¯n ngÆ°á»i dÃ¹ng
    messages.append({"role": "user", "content": user_input})

    # Sinh pháº£n há»“i
    output = pipe(
        messages,
        max_new_tokens=250,      # giáº£m Ä‘á»ƒ nhanh hÆ¡n
        do_sample=False,
        return_full_text=False,
    )

    response = output[0]['generated_text'].strip()
    print(f"ğŸ¤– Bot: {response}")

    # LÆ°u pháº£n há»“i vÃ o lá»‹ch sá»­ (Ä‘á»ƒ tiáº¿p tá»¥c há»™i thoáº¡i)
    messages.append({"role": "assistant", "content": response})